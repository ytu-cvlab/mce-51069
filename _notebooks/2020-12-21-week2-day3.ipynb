{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2, Day 3 (Coding the first Convolutional Neural Network)\n",
    "> Welcome to third day (Week 2) of the McE-51069 course. We will walk you through with the basic flow of building the Convolutional Neural Network.\n",
    "\n",
    "- sticky_rank: 5\n",
    "- toc: true\n",
    "- badges: true\n",
    "- comments: false\n",
    "- categories: [Pytorch, Convolutional_Neural_Network]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KgJGtoDugBEx"
   },
   "source": [
    "# Import Necessary Libraries\n",
    "\n",
    "Pytorch is the open source machine learning framework that we can use for research to production.\n",
    "\n",
    "If you would like to study the tutorial from the offical [pytorch](https://pytorch.org/) website, please visit [this link](https://pytorch.org/tutorials/).\n",
    "\n",
    "The source code for the entire pytorch framework can be found [here](https://github.com/pytorch).\n",
    "\n",
    "> SideNote: pytorch seperate different task into different module. eg. there is a package called **torchaudio** that focus on audio alone.\n",
    "\n",
    "\n",
    "Today we will use torchvision, which is the package inside torch, that focus on vision task. For example, for data augmentation, torchvision provide the function called **transforms**. And if you would like to use transfer learning, torchvision also provide some of the state of the art model pretrained available.\n",
    "\n",
    "torch.nn contain the basic building blocks that we need to construct our model.<br>\n",
    "For example, if we need to construct a **Convolutional Layer**, we could call the function of `torch.nn.Conv2d()` to construct that layer.\n",
    "\n",
    "And if we want linear layer that perform the equation of $y = W^TX + b$, we could call for the function of `torch.nn.Linear()`.\n",
    "\n",
    "If you would like to know more about `torch.nn` library, please visit this [link](https://pytorch.org/docs/stable/nn.html) for more information. Also, this [post](https://pytorch.org/tutorials/beginner/nn_tutorial.html) provide better understanding for `torch.nn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 6789,
     "status": "ok",
     "timestamp": 1608533477149,
     "user": {
      "displayName": "Aung Paing",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtNjXDzCg0wqagLdTo1q7N18g1ys6GIxknFcJN=s64",
      "userId": "03851297839959231119"
     },
     "user_tz": -390
    },
    "id": "wejqfkizlvnD"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_uDlHmO-pCWH"
   },
   "source": [
    "# Main Components\n",
    "\n",
    "Let's preview what will be built in this notebook.\n",
    "\n",
    " 1. Dataset \n",
    "  - cifar10\n",
    " 2. Model Architecture\n",
    "  - Simple Model\n",
    " 3. Loss (Update model)\n",
    "  - Categorical Loss Entropy\n",
    " 4. Optimizer (Regularizer)\n",
    "  - Adam Optimizer\n",
    " 5. Metrics (Visual for User)\n",
    "  - Loss, Accuracy\n",
    " 6. Save Model\n",
    "  - Model Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zmfg8ySjn0D1"
   },
   "source": [
    "If you would like to change from CPU to GPU,\n",
    "Select the `Runtime --> Change Runtime type` and select `GPU`.\n",
    "\n",
    "After done selecting, we can check whether we are running GPU or CPU with the following function.\n",
    "\n",
    "If the output show `cuda:0`, then the `GPU` is in used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3088,
     "status": "ok",
     "timestamp": 1608533477160,
     "user": {
      "displayName": "Aung Paing",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtNjXDzCg0wqagLdTo1q7N18g1ys6GIxknFcJN=s64",
      "userId": "03851297839959231119"
     },
     "user_tz": -390
    },
    "id": "xfhNywLHWTco",
    "outputId": "5f6d39ca-5b07-4e32-a710-448d4c7529b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Bvkx_6YoXM0"
   },
   "source": [
    "# Dataset\n",
    "\n",
    "In this notebook, we will use `CIFAR10` dataset for classification. \n",
    "\n",
    "[CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html) is a public classification dataset, which consists of 60,000 32 * 32 color images in 10 classes. There are 50,000 training images and 10,000 testing images. \n",
    "\n",
    "In the following cell, we will do data augmentation for the dataset. When doing **trasnforms** on the test dataset, we only Normalize the input, that is because we do not need to augment(change) our test dataset to see the result.\n",
    "\n",
    "Tensor is just like Numpy's ndarray, except that it can done calculation on GPU, and is modified to fit the training neural network procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121,
     "referenced_widgets": [
      "1d49b12207ba4585af31705400e3e08a",
      "ddac44ab31ea4b679f9b4a2763db83fe",
      "4dc3edc401ed49afbd8d191dac962a4d",
      "31b3eba853914d2d8fd7653949352f21",
      "2aed722ad1434ee0b61668b75a9c5f4f",
      "45b811ca63f5474fab7967950d8840f5",
      "de04880858954d1bbb7e16393f1d4f36",
      "14645812df274c0e85ac0a042d660ff3"
     ]
    },
    "executionInfo": {
     "elapsed": 12533,
     "status": "ok",
     "timestamp": 1608533486616,
     "user": {
      "displayName": "Aung Paing",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtNjXDzCg0wqagLdTo1q7N18g1ys6GIxknFcJN=s64",
      "userId": "03851297839959231119"
     },
     "user_tz": -390
    },
    "id": "QOOHZDMPo1UV",
    "outputId": "6bc86b1d-3365-45d9-eec0-d0baad699c81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d49b12207ba4585af31705400e3e08a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# The class names for CIFAR 10\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "\n",
    "# Image Normalization, Data Augmentation\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,)),\n",
    "                                transforms.RandomRotation(30)\n",
    "                                ])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "        './data', train=True,\n",
    "        transform=transform, download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "        './data', train=False,\n",
    "        transform=test_transform, download=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "afcYs8kvvkEu"
   },
   "source": [
    "> Note: Pytorch store the image dataset in the following format:\n",
    "`batch_size * dim * height * width (B*D*H*W)`.\n",
    "\n",
    "Batch_size mean Number of images for one training iteration.\n",
    "Dim mean the image dimension. Conventionally, color image have the dimension of *3* and gray scale image have the dimension of *1*.\n",
    "\n",
    "The label value for the dataset range from `0~9`. eg. If the label is *7*, then, it would be **class_names[7] = horse**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 12528,
     "status": "ok",
     "timestamp": 1608533486620,
     "user": {
      "displayName": "Aung Paing",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtNjXDzCg0wqagLdTo1q7N18g1ys6GIxknFcJN=s64",
      "userId": "03851297839959231119"
     },
     "user_tz": -390
    },
    "id": "QxHzBG67pDVR"
   },
   "outputs": [],
   "source": [
    "# Code Block\n",
    "\n",
    "# Check the total number of images in train_dataset\n",
    "\n",
    "\n",
    "# Plot training Image and print label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xU-e7QXcx1Jp"
   },
   "source": [
    "## Dataset and DataLoader\n",
    "\n",
    "Dataset play the huge role in machine learning and deep learning. For the computer vision task, we could do data augmentation to get the effect of regulating the model, avoid being overfitting. And when training, the augmented dataset need to be fetched by the data loader. The main duty of the dataloader is to prepare dataset before feeding to the neural network.\n",
    "\n",
    "Dataloader object is a generator object, which can be accessed through iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 12522,
     "status": "ok",
     "timestamp": 1608533486621,
     "user": {
      "displayName": "Aung Paing",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtNjXDzCg0wqagLdTo1q7N18g1ys6GIxknFcJN=s64",
      "userId": "03851297839959231119"
     },
     "user_tz": -390
    },
    "id": "fl5zipzpsfMO"
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, 32, shuffle=True,\n",
    "                                           num_workers=2)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, 8, shuffle=True,\n",
    "                                           num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 12515,
     "status": "ok",
     "timestamp": 1608533486622,
     "user": {
      "displayName": "Aung Paing",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtNjXDzCg0wqagLdTo1q7N18g1ys6GIxknFcJN=s64",
      "userId": "03851297839959231119"
     },
     "user_tz": -390
    },
    "id": "vRhgs8a8115y"
   },
   "outputs": [],
   "source": [
    "# Code Snipptets 2\n",
    "\n",
    "# Explain Generator\n",
    "\n",
    "# Access train_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EQU-uJSh2xu4"
   },
   "source": [
    "# Model\n",
    "\n",
    "Let's built a simple Convolutional Neural Network.\n",
    "\n",
    "When flattening the network, we can use the equation:<br>\n",
    "$O = \\frac{W-K+2P}{S} + 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 12503,
     "status": "ok",
     "timestamp": 1608533486623,
     "user": {
      "displayName": "Aung Paing",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtNjXDzCg0wqagLdTo1q7N18g1ys6GIxknFcJN=s64",
      "userId": "03851297839959231119"
     },
     "user_tz": -390
    },
    "id": "5PAj_NegsfOi"
   },
   "outputs": [],
   "source": [
    "class SampleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SampleModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3)\n",
    "        self.conv2 = nn.Conv2d(32, 128, 3)\n",
    "        self.fc1 = nn.Linear(28*28*128, 512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(-1, 28*28*128)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 23465,
     "status": "ok",
     "timestamp": 1608533497592,
     "user": {
      "displayName": "Aung Paing",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtNjXDzCg0wqagLdTo1q7N18g1ys6GIxknFcJN=s64",
      "userId": "03851297839959231119"
     },
     "user_tz": -390
    },
    "id": "VYI-5pmqsfQ8"
   },
   "outputs": [],
   "source": [
    "model = SampleModel()\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# Define Loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Define Optimizer\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 927,
     "status": "ok",
     "timestamp": 1608530599029,
     "user": {
      "displayName": "Aung Paing",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtNjXDzCg0wqagLdTo1q7N18g1ys6GIxknFcJN=s64",
      "userId": "03851297839959231119"
     },
     "user_tz": -390
    },
    "id": "WgltXAFo3zhe",
    "outputId": "223063c1-5107-4124-a6bb-9cf8c0fe6735"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0592, -0.0659,  0.0515,  0.0013,  0.0012, -0.0716, -0.1198,  0.0276,\n",
       "         -0.0834, -0.0727]], device='cuda:0', grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 98,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code Snippets 3\n",
    "\n",
    "# Test Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RQmTHy_7SMej"
   },
   "outputs": [],
   "source": [
    "EPOCH = 10\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    loss_epoch = 0\n",
    "    total_imgs = 0\n",
    "    correct = 0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        imgs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # Clear-out Gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Predict from the input images\n",
    "        predicts = model(imgs)\n",
    "\n",
    "        # Calculate Loss\n",
    "        loss = criterion(predicts, labels)\n",
    "\n",
    "        # Calculate Gradient\n",
    "        loss.backward()\n",
    "        # Update Model Parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # log loss for evaluate model training performance.\n",
    "        loss_epoch += loss.item()\n",
    "\n",
    "        total_imgs += len(imgs)\n",
    "        correct += (torch.argmax(predicts, 1)==labels).sum().item()\n",
    "    \n",
    "    acc = (correct/total_imgs)*100\n",
    "    print(f\"EPOCH : {epoch}, Acc : {acc:.2f}, Loss : {loss:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p2qAJZl76dyX"
   },
   "outputs": [],
   "source": [
    "# Code Snippets 4\n",
    "# Play with model.state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hf6RjLLV6T5P"
   },
   "outputs": [],
   "source": [
    "# Saving Model\n",
    "\n",
    "# Checkpoint\n",
    "ckpt_path = 'checkpoint.pt'\n",
    "\n",
    "torch.save(model.state_dict(), ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oonH7HONS8Q2"
   },
   "outputs": [],
   "source": [
    "# Save the model to drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hZvharuur9Vf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1073,
     "status": "ok",
     "timestamp": 1608514209780,
     "user": {
      "displayName": "Aung Paing",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtNjXDzCg0wqagLdTo1q7N18g1ys6GIxknFcJN=s64",
      "userId": "03851297839959231119"
     },
     "user_tz": -390
    },
    "id": "thAarnq5azZQ",
    "outputId": "d502a9e1-91d9-443c-a572-092223734367"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SampleModel().to(device)\n",
    "checkpoint = torch.load(ckpt_path)\n",
    "model.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "10nZFQNQw9U1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6859,
     "status": "ok",
     "timestamp": 1608523263434,
     "user": {
      "displayName": "Aung Paing",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtNjXDzCg0wqagLdTo1q7N18g1ys6GIxknFcJN=s64",
      "userId": "03851297839959231119"
     },
     "user_tz": -390
    },
    "id": "m9tiqgIiiqbr",
    "outputId": "8cc597b5-9e1f-4b00-c7c5-f04d61d19e30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7093 10000\n"
     ]
    }
   ],
   "source": [
    "# Test Dataset accuracy:\n",
    "num_total = 0\n",
    "num_correct = 0\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader):\n",
    "        image, label = data\n",
    "        image = image.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        predict = model(image)\n",
    "        predict_class = torch.argmax(predict, 1)\n",
    "        correct = (predict_class == label)\n",
    "\n",
    "\n",
    "        num_correct += correct.sum().item()\n",
    "        num_total += len(image)\n",
    "        \n",
    "print(num_correct, num_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6606,
     "status": "ok",
     "timestamp": 1608523263435,
     "user": {
      "displayName": "Aung Paing",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtNjXDzCg0wqagLdTo1q7N18g1ys6GIxknFcJN=s64",
      "userId": "03851297839959231119"
     },
     "user_tz": -390
    },
    "id": "VSVOWVrviqh5",
    "outputId": "59d765ea-0826-42c9-db56-8b791469e7ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.7093\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy : \", num_correct/num_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U3yi0RJba5V6"
   },
   "outputs": [],
   "source": [
    "# Helper Function to show your testing accuracy.\n",
    "\n",
    "def test_model(model):\n",
    "    data = next(iter(test_loader))\n",
    "    imgs, labels = data[0].to(device), data[1]\n",
    "    predicts = model(imgs)\n",
    "    print(predicts.size())\n",
    "    index = torch.argmax(predicts, 1)\n",
    "    titles = []\n",
    "    for i in index:\n",
    "        titles.append(class_names[i])\n",
    "    plt.figure(figsize=(20,10))\n",
    "    for i in range(len(titles)):\n",
    "        title = f\"Predict : {titles[i]}, Actual : {class_names[labels[i]]}\"\n",
    "        if titles[i]==class_names[labels[i]]:color = 'blue'\n",
    "        else:color='red'\n",
    "        img = (imgs[i].cpu().numpy()/2)+0.5\n",
    "        plt.subplot(2, 4, i+1)\n",
    "        plt.imshow(img.transpose(1, 2, 0));plt.title(title, fontdict={'fontsize':17, 'color':color})\n",
    "\n",
    "test_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kaCCti1nElyH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1196,
     "status": "ok",
     "timestamp": 1608534502176,
     "user": {
      "displayName": "Aung Paing",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtNjXDzCg0wqagLdTo1q7N18g1ys6GIxknFcJN=s64",
      "userId": "03851297839959231119"
     },
     "user_tz": -390
    },
    "id": "zoRoXlCvG_-h",
    "outputId": "56af4dcd-035a-4d25-e193-6a718b4769f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "# Checking the model weights size\n",
    "# for keys in model.state_dict().keys():\n",
    "#     print(model.state_dict()[keys][0].size())\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1895,
     "status": "ok",
     "timestamp": 1608534906438,
     "user": {
      "displayName": "Aung Paing",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtNjXDzCg0wqagLdTo1q7N18g1ys6GIxknFcJN=s64",
      "userId": "03851297839959231119"
     },
     "user_tz": -390
    },
    "id": "PTNURzYkHABx",
    "outputId": "b9dcf26f-5e5b-41bc-b0b4-1f68a0f792a2"
   },
   "outputs": [],
   "source": [
    "# for para in model.parameters():\n",
    "#     print(para)\n",
    "#     break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BSGcXlS1BuaP"
   },
   "outputs": [],
   "source": [
    "# # Code Snippet 1\n",
    "\n",
    "# print(len(train_dataset))\n",
    "\n",
    "# # img_ind ~ 0~4,999\n",
    "# # \n",
    "# img_ind = 60\n",
    "# image = train_dataset[img_ind][0].numpy()\n",
    "# label = train_dataset[img_ind][1]\n",
    "\n",
    "# image = image.transpose(1, 2, 0)\n",
    "# print(class_names[label])\n",
    "# plt.imshow(image)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QrB5uUBM2lDH"
   },
   "outputs": [],
   "source": [
    "# # Code Snipptets 2\n",
    "\n",
    "# # Explain Generator\n",
    "\n",
    "# # Access train_loader\n",
    "# for data in train_loader:\n",
    "#     print(data[0].size())\n",
    "#     print(data[1].size())\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GZUtRaNa5XlP"
   },
   "outputs": [],
   "source": [
    "# # Code Snippets 3\n",
    "\n",
    "# # Test Model\n",
    "# input_images = torch.rand(1, 3, 32, 32).to(device)\n",
    "# prediction = model(input_images)\n",
    "# prediction"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPTRoTvtLK5E4dMB25z9Ifu",
   "collapsed_sections": [],
   "mount_file_id": "1icAuSJPkHMFKbNpiM6x2dkemq-AgJrw3",
   "name": "Day_3_notebook.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "09f26b1a826a48b790f6660f22d7bbec": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6c0e0b704cbf4411ba781231dc24afe5",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_b12ba102827b4f97ae9a27f517e5e6f5",
      "value": " 170500096/? [00:20&lt;00:00, 95895147.07it/s]"
     }
    },
    "6c0e0b704cbf4411ba781231dc24afe5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "74089fdfe87f414d9906585f8897ee53": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "92f3db5f870946ed9d850598f765871a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a19b0c1d212345bda712bae2191285ff": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "b12ba102827b4f97ae9a27f517e5e6f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bd729cb627624705803b066729bee9ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_92f3db5f870946ed9d850598f765871a",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a19b0c1d212345bda712bae2191285ff",
      "value": 1
     }
    },
    "f69c551e1ea84d869f00409574f3d7cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bd729cb627624705803b066729bee9ab",
       "IPY_MODEL_09f26b1a826a48b790f6660f22d7bbec"
      ],
      "layout": "IPY_MODEL_74089fdfe87f414d9906585f8897ee53"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
